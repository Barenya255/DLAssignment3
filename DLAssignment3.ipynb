{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4p64ik20LTQW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.auto import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqcYOBpdKqnz",
        "outputId": "4072643f-23f2-4ef2-c1b1-6c4a6d343f02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: gdown\n",
            "unzip:  cannot find or open aksharantar_sampled.zip, aksharantar_sampled.zip.zip or aksharantar_sampled.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!gdown \"1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw\"\n",
        "!unzip -q aksharantar_sampled.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5CuhFISFKvrw"
      },
      "outputs": [],
      "source": [
        "class PrepText():\n",
        "    def __init__ (self, maxSize):\n",
        "        self.textToNumX = {}\n",
        "        self.numToTextX = {}\n",
        "        self.textToNumY = {}\n",
        "        self.numToTextY = {}\n",
        "        self.encodingLength = maxSize\n",
        "\n",
        "\n",
        "    def makeDict(self, wordsX, wordsY):\n",
        "        #print (\"creating the dictionary.\")\n",
        "\n",
        "\n",
        "        self.textToNumX[\"PAD\"] = 0\n",
        "        self.textToNumX[\"SOS\"] = 1\n",
        "        self.textToNumX[\"EOS\"] = 2\n",
        "        self.count = 3\n",
        "        for word in wordsX:\n",
        "            for letter in word:\n",
        "                if letter not in self.textToNumX:\n",
        "                    self.textToNumX[letter] = self.count\n",
        "                    self.count+=1\n",
        "\n",
        "        \n",
        "        for letter, number in self.textToNumX.items():\n",
        "            self.numToTextX[number] = letter\n",
        "\n",
        "        self.textToNumY[\"PAD\"] = 0\n",
        "        self.textToNumY[\"SOS\"] = 1\n",
        "        self.textToNumY[\"EOS\"] = 2\n",
        "        self.count = 3\n",
        "        for word in wordsY:\n",
        "            for letter in word:\n",
        "                if letter not in self.textToNumY:\n",
        "                    self.textToNumY[letter] = self.count\n",
        "                    self.count+=1\n",
        "\n",
        "        \n",
        "        for letter, number in self.textToNumY.items():\n",
        "            self.numToTextY[number] = letter\n",
        "\n",
        "    \n",
        "    def lenOutput(self):\n",
        "        return len(self.numToTextY);\n",
        "\n",
        "\n",
        "    def lenInput(self):\n",
        "        return len(self.numToTextX);\n",
        "\n",
        "        \n",
        "    def vectorizeOneWord(self, wordX, wordY):\n",
        "        self.vectorX = torch.zeros(self.encodingLength, dtype = torch.int)\n",
        "        self.vectorY = torch.zeros(self.encodingLength, dtype = torch.int)\n",
        "\n",
        "\n",
        "        #print(\"encoding english word: \" + wordX + \" encoding hindi word: \" + wordY)\n",
        "\n",
        "        self.count = 1\n",
        "        self.vectorX[0] = self.textToNumX['SOS']\n",
        "        for letter in wordX:\n",
        "            self.vectorX[self.count] = self.textToNumX[letter]\n",
        "            self.count += 1\n",
        "        self.vectorX[self.count] = self.textToNumX['EOS']\n",
        "\n",
        "\n",
        "\n",
        "        self.count = 1\n",
        "        self.vectorY[0] = self.textToNumY['SOS']\n",
        "        for letter in wordY:\n",
        "            self.vectorY[self.count] = self.textToNumY[letter]\n",
        "            self.count += 1\n",
        "        self.vectorY[self.count] = self.textToNumY['EOS']\n",
        "\n",
        "        return self.vectorX, self.vectorY\n",
        "\n",
        "    def vectorToWord (self, vectorA, vectorB):\n",
        "        wordA = \"\"\n",
        "        wordB = \"\"\n",
        "\n",
        "\n",
        "        for element in vectorA:\n",
        "            wordA += self.textToNumX[element.item()]\n",
        "\n",
        "        for element in vectorB:\n",
        "            wordB += self.textToNumY[element.item()]\n",
        "\n",
        "        \n",
        "        return wordA, wordB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LE4augyAXUtG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class AksharantarData(Dataset):\n",
        "\n",
        "    def __init__(self, rootPath, max_size):\n",
        "\n",
        "        self.root  = rootPath\n",
        "        self.df = pd.read_csv(self.root, names = [\"english\", \"hindi\"])\n",
        "\n",
        "\n",
        "        self.english = self.df[\"english\"]\n",
        "        self.hindi = self.df[\"hindi\"]\n",
        "\n",
        "\n",
        "        self.vocab = PrepText(max_size)\n",
        "        self.vocab.makeDict(self.english, self.hindi)\n",
        "\n",
        "\n",
        "    def lenOutput(self):\n",
        "        return self.vocab.lenOutput()\n",
        "\n",
        "\n",
        "    def lenInput(self):\n",
        "        return self.vocab.lenInput()\n",
        "\n",
        "    def getDictEng (self):\n",
        "        return self.vocab.textToNumX;\n",
        "\n",
        "    def getDictHin (self):\n",
        "        return self.vocab.textToNumY;\n",
        "\n",
        "    \n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.df)\n",
        "\n",
        "\n",
        "    def __getitem__ (self, idx):\n",
        "\n",
        "        #print(idx)\n",
        "\n",
        "        self.englishWord = self.english[idx]\n",
        "        #print(self.englishWord)\n",
        "        self.hindiWord = self.hindi[idx]\n",
        "        #print(self.hindiWord)\n",
        "        self.vecEncodedX, self.vecEncodedY = self.vocab.vectorizeOneWord(self.englishWord, self.hindiWord)\n",
        "        return (self.vecEncodedX, self.vecEncodedY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lSY4bQ3rasNf"
      },
      "outputs": [],
      "source": [
        "trainData = AksharantarData(\"/content/aksharantar_sampled/hin/hin_train.csv\", 35)\n",
        "\n",
        "\n",
        "# update if necessary\n",
        "valData = AksharantarData(\"/content/aksharantar_sampled/hin/hin_valid.csv\", 35) \n",
        "# update if necessary\n",
        "testData = AksharantarData(\"/content/aksharantar_sampled/hin/hin_test.csv\", 35)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "od2P0wzkui41"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# declare the batch size.\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# train data loader.\n",
        "trainloader = DataLoader(trainData, shuffle = True, batch_size = BATCH_SIZE)\n",
        "\n",
        "# validation data loader.\n",
        "valLoader = DataLoader(valData, shuffle = True, batch_size = BATCH_SIZE)\n",
        "\n",
        "# test data loader.\n",
        "testLoader = DataLoader(testData, shuffle = True, batch_size = BATCH_SIZE)\n",
        "\n",
        "# currently set it to false for debugging purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AhKM9UppQUC0"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9ynQ3Foav_M4"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (seq_length, N) where N is batch size\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        # embedding shape: (seq_length, N, embedding_size)\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedding)\n",
        "        # outputs shape: (seq_length, N, hidden_size)\n",
        "\n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mU2CDm0i3BEW"
      },
      "outputs": [],
      "source": [
        "class Decoder (nn.Module):\n",
        "    def __init__ (self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
        "        super (Decoder, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "\n",
        "        self.embedding_layer = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "        self.fc = nn.Linear (hidden_size, output_size)\n",
        "\n",
        "\n",
        "    def forward (self, X, hidden, cell):\n",
        "\n",
        "        X = X.unsqueeze(0)\n",
        "        \n",
        "\n",
        "        embedded = self.embedding_layer(X)\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        predictions = self.fc(output)\n",
        "        predictions = predictions.squeeze(0)\n",
        "\n",
        "\n",
        "        return predictions, hidden, cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "05tLjZQAQLU5"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
        "        # is 1 here because we are sending in a single word and not a sentence\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "        embedding = self.dropout(self.embedding(x))\n",
        "        # embedding shape: (1, N, embedding_size)\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "        # outputs shape: (1, N, hidden_size)\n",
        "\n",
        "        predictions = self.fc(outputs)\n",
        "\n",
        "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
        "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
        "        # just gonna remove the first dim\n",
        "        predictions = predictions.squeeze(0)\n",
        "\n",
        "        return predictions, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-1dBqH6yctlW"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
        "        batch_size = source.shape[1]\n",
        "        target_len = target.shape[0]\n",
        "        target_vocab_size = trainData.lenOutput()+1\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
        "\n",
        "        hidden, cell = self.encoder(source)\n",
        "        #print(\"here lays hidden.shape\")\n",
        "        #print(hidden.shape)\n",
        "        # Grab the first input to the Decoder which will be <SOS> token\n",
        "        x = target[0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            # Use previous hidden, cell as context from encoder at start\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "\n",
        "            # Store next output prediction\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
        "            best_guess = output.argmax(1)\n",
        "\n",
        "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
        "\n",
        "        return outputs\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4hC7y4s0ZHPD"
      },
      "outputs": [],
      "source": [
        "# Training hyper parameters\n",
        "batchSize = 128\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "inputSizeEncoder = trainData.lenInput()+1\n",
        "inputSizeDecoder = trainData.lenOutput()+1\n",
        "outputSize = trainData.lenOutput()+1\n",
        "encoderEmbedding = 300\n",
        "decoderEmbedding = 300\n",
        "hiddenSize = 1024\n",
        "numLayers = 8\n",
        "encDropout = 0.5\n",
        "decDropout = 0.5\n",
        "num_epochs = 10\n",
        "learningRate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eHIvkjJ-fDs6"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder (inputSizeEncoder, encoderEmbedding, hiddenSize, numLayers, encDropout).to(device)\n",
        "decoder = Decoder (inputSizeDecoder, decoderEmbedding,  hiddenSize, outputSize, numLayers, decDropout).to(device)\n",
        "model = EncoderDecoder(encoder, decoder).to(device)\n",
        "criterion = nn.CrossEntropyLoss(reduction = \"sum\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2eIWkHNFFlN",
        "outputId": "fb1b6053-1e36-459b-a8c7-d819187f46ee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method Module.parameters of EncoderDecoder(\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(29, 300)\n",
              "    (rnn): LSTM(300, 1024, num_layers=8, dropout=0.5)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (embedding): Embedding(67, 300)\n",
              "    (rnn): LSTM(300, 1024, num_layers=8, dropout=0.5)\n",
              "    (fc): Linear(in_features=1024, out_features=67, bias=True)\n",
              "  )\n",
              ")>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439,
          "referenced_widgets": [
            "2a29b6d6b3c6404f9d89b58c741e3d0b",
            "429ff2decd894ca0a80323d576ed354f",
            "cc2346a26f534c0bbef976fd00dccf2f",
            "ac3f2918ab7d4936b20ffcce7f63e940",
            "2a4ab8d1e2724e7284e71cd606ae0660",
            "1a341c7395f3408ca32582d7995c1af3",
            "a0ef50ef5d1441e7bd0d7e633c86cbdc",
            "864ad492869041b39ebea2b7d9664e3e",
            "86f42a0da23e4ea492a3496575e2d8bb",
            "9d3f837a0ace4a97aec0ed49aa4b0269",
            "8439dbdb6f5a4219aea35d1711b0cfa2"
          ]
        },
        "id": "Y4F2jRjmhcoV",
        "outputId": "42fe61ca-df5b-4f07-d822-735f3b9d5e60"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a29b6d6b3c6404f9d89b58c741e3d0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/10\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4693e6d33890>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-0b42a87f4d7d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, target, teacher_force_ratio)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m#print(\"here lays hidden.shape\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#print(hidden.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-567e863b75f2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# embedding shape: (seq_length, N, embedding_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# outputs shape: (seq_length, N, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    813\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    814\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    trainAcc = 0.0\n",
        "    trainLoss = 0.0\n",
        "\n",
        "    print (f'Epoch {epoch}/{num_epochs}')\n",
        "    model.train()\n",
        "    batchNo = 0\n",
        "    \n",
        "    for x,y in trainloader:\n",
        "        #print (f\"Doing batch Number: {batchNo}\")\n",
        "        batchNo+=1\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        \n",
        "        output = model(x, y)\n",
        "        \n",
        "\n",
        "        output = output.reshape(-1, output.shape[2])\n",
        "\n",
        "        y = y.reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, y.to(torch.long))\n",
        "        trainLoss += loss\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) # avoid exploding gradient problem\n",
        "        optimizer.step()\n",
        "\n",
        "        trainAcc += wordAccuracy (output.argmax(1), y.to(torch.long), batchSize)\n",
        "\n",
        "        #print(loss)\n",
        "        \n",
        "\n",
        "        if batchNo % 100 == 0:\n",
        "            print(f\"After {batchNo} batches,========\")\n",
        "            break\n",
        "    trainLoss /= (51200*35)\n",
        "    #trainLoss *= 35\n",
        "    trainAcc /= (51200*35) \n",
        "    #trainAcc *= 35\n",
        "    print(f\"Training Loss : {trainLoss}\")\n",
        "    print(f\"Training accuracy : {trainAcc}\")\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    valAcc = 0.0\n",
        "    valLoss = 0.0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for x,y in valLoader: \n",
        "            x,y = x.to(device), y.to(device)\n",
        "\n",
        "            output = model(x,y)\n",
        "\n",
        "            output = output.reshape(-1, output.shape[2])\n",
        "            #print(y.shape)\n",
        "            y = y.reshape(-1)\n",
        "\n",
        "            loss = criterion (output, y.to(torch.long))\n",
        "            valAcc += wordAccuracy (output.argmax(1), y.to(torch.long), batchSize)\n",
        "            valLoss += loss\n",
        "\n",
        "        valLoss /= len(valLoader)*batchSize*35\n",
        "        valAcc /= len(valLoader)*batchSize*35\n",
        "\n",
        "\n",
        "        print(f\"Validation Loss = {valLoss} and Validation accuracy = {valAcc}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq9yrLvJhQtc",
        "outputId": "a4e48a66-5865-4ea9-9b67-4ad53297967b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4096"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(valLoader)*batchSize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vVJ1YZ_tNEPy"
      },
      "outputs": [],
      "source": [
        "def wordAccuracy (prediction, target, batchSize):\n",
        "\n",
        "\n",
        "    answer = 0\n",
        "    for i in range(batchSize):\n",
        "        answer += torch.equal(prediction[i], target[i])\n",
        "\n",
        "    return answer"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a341c7395f3408ca32582d7995c1af3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a29b6d6b3c6404f9d89b58c741e3d0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_429ff2decd894ca0a80323d576ed354f",
              "IPY_MODEL_cc2346a26f534c0bbef976fd00dccf2f",
              "IPY_MODEL_ac3f2918ab7d4936b20ffcce7f63e940"
            ],
            "layout": "IPY_MODEL_2a4ab8d1e2724e7284e71cd606ae0660"
          }
        },
        "2a4ab8d1e2724e7284e71cd606ae0660": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "429ff2decd894ca0a80323d576ed354f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a341c7395f3408ca32582d7995c1af3",
            "placeholder": "​",
            "style": "IPY_MODEL_a0ef50ef5d1441e7bd0d7e633c86cbdc",
            "value": "  0%"
          }
        },
        "8439dbdb6f5a4219aea35d1711b0cfa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "864ad492869041b39ebea2b7d9664e3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86f42a0da23e4ea492a3496575e2d8bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d3f837a0ace4a97aec0ed49aa4b0269": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0ef50ef5d1441e7bd0d7e633c86cbdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac3f2918ab7d4936b20ffcce7f63e940": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d3f837a0ace4a97aec0ed49aa4b0269",
            "placeholder": "​",
            "style": "IPY_MODEL_8439dbdb6f5a4219aea35d1711b0cfa2",
            "value": " 0/10 [00:19&lt;?, ?it/s]"
          }
        },
        "cc2346a26f534c0bbef976fd00dccf2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_864ad492869041b39ebea2b7d9664e3e",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86f42a0da23e4ea492a3496575e2d8bb",
            "value": 0
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
